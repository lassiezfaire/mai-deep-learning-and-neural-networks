{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Реализуйте алгоритм SAC для среды lunar lander"
      ],
      "metadata": {
        "id": "Sr7dwOCeR0oU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install swig\n",
        "!pip install \"gymnasium[box2d]\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hjQrXMAXg702",
        "outputId": "3bc2f8b0-9f88-454a-e456-f99154c27843"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting swig\n",
            "  Downloading swig-4.3.1-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (3.5 kB)\n",
            "Downloading swig-4.3.1-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.9 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.9 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.8/1.9 MB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: swig\n",
            "Successfully installed swig-4.3.1\n",
            "Requirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.11/dist-packages (1.1.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (4.13.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (0.0.4)\n",
            "Collecting box2d-py==2.3.5 (from gymnasium[box2d])\n",
            "  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (2.6.1)\n",
            "Requirement already satisfied: swig==4.* in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (4.3.1)\n",
            "Building wheels for collected packages: box2d-py\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp311-cp311-linux_x86_64.whl size=2379418 sha256=634e054488e1b5177d88ed9675605cd716dba2d245f47c82b004cf3a745e5fdb\n",
            "  Stored in directory: /root/.cache/pip/wheels/ab/f1/0c/d56f4a2bdd12bae0a0693ec33f2f0daadb5eb9753c78fa5308\n",
            "Successfully built box2d-py\n",
            "Installing collected packages: box2d-py\n",
            "Successfully installed box2d-py-2.3.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from collections import deque\n",
        "import random\n",
        "from torch.distributions import Normal"
      ],
      "metadata": {
        "id": "MN6a51jLg8Ge"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GAMMA = 0.99\n",
        "TAU = 0.005\n",
        "ALPHA = 0.2\n",
        "ACTOR_LR = 3e-4\n",
        "CRITIC_LR = 3e-4\n",
        "REPLAY_SIZE = 100000\n",
        "BATCH_SIZE = 256\n",
        "START_STEPS = 10000\n",
        "TOTAL_STEPS = 200000\n",
        "UPDATE_AFTER = 1000\n",
        "UPDATE_EVERY = 50"
      ],
      "metadata": {
        "id": "OrmQtoBmg-Cu"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "tfzBEkl4h3Lv"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Actor(nn.Module):\n",
        "    def __init__(self, obs_dim, act_dim, action_low, action_high):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(obs_dim, 256), nn.ReLU(),\n",
        "            nn.Linear(256, 256), nn.ReLU()\n",
        "        )\n",
        "        self.mu_layer = nn.Linear(256, act_dim)\n",
        "        self.log_std_layer = nn.Linear(256, act_dim)\n",
        "        self.action_low = torch.tensor(action_low, device=device)\n",
        "        self.action_high = torch.tensor(action_high, device=device)\n",
        "\n",
        "    def forward(self, obs):\n",
        "        x = self.net(obs)\n",
        "        mean = self.mu_layer(x)\n",
        "        log_std = self.log_std_layer(x)\n",
        "        log_std = torch.clamp(log_std, -20, 2)\n",
        "        std = log_std.exp()\n",
        "\n",
        "        normal = Normal(mean, std)\n",
        "        x_t = normal.rsample()\n",
        "        y_t = torch.tanh(x_t)\n",
        "        action = y_t * (self.action_high - self.action_low)/2 + (self.action_high + self.action_low)/2\n",
        "\n",
        "        log_prob = normal.log_prob(x_t)\n",
        "        log_prob -= torch.log((1 - y_t.pow(2)) + 1e-6)\n",
        "        log_prob = log_prob.sum(1, keepdim=True)\n",
        "\n",
        "        return action, log_prob\n",
        "\n",
        "    def get_action(self, obs, deterministic=False):\n",
        "        with torch.no_grad():\n",
        "            obs_tensor = torch.FloatTensor(obs).to(device).unsqueeze(0)\n",
        "            x = self.net(obs_tensor)\n",
        "            mean = self.mu_layer(x)\n",
        "            if deterministic:\n",
        "                action = torch.tanh(mean)\n",
        "            else:\n",
        "                log_std = self.log_std_layer(x)\n",
        "                log_std = torch.clamp(log_std, -20, 2)\n",
        "                std = log_std.exp()\n",
        "                normal = Normal(mean, std)\n",
        "                x_t = normal.rsample()\n",
        "                action = torch.tanh(x_t)\n",
        "            action = action * (self.action_high - self.action_low)/2 + (self.action_high + self.action_low)/2\n",
        "            return action.squeeze().cpu().numpy()"
      ],
      "metadata": {
        "id": "iPYOjK6Gg_tW"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Critic(nn.Module):\n",
        "    def __init__(self, obs_dim, act_dim):\n",
        "        super().__init__()\n",
        "        self.q1 = nn.Sequential(\n",
        "            nn.Linear(obs_dim + act_dim, 256), nn.ReLU(),\n",
        "            nn.Linear(256, 256), nn.ReLU(),\n",
        "            nn.Linear(256, 1)\n",
        "        )\n",
        "        self.q2 = nn.Sequential(\n",
        "            nn.Linear(obs_dim + act_dim, 256), nn.ReLU(),\n",
        "            nn.Linear(256, 256), nn.ReLU(),\n",
        "            nn.Linear(256, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, obs, act):\n",
        "        x = torch.cat([obs, act], dim=-1)\n",
        "        return self.q1(x), self.q2(x)"
      ],
      "metadata": {
        "id": "A0jBQNi1hBJ2"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, size):\n",
        "        self.buffer = deque(maxlen=size)\n",
        "\n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        self.buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        batch = random.sample(self.buffer, batch_size)\n",
        "        states, actions, rewards, next_states, dones = zip(*batch)\n",
        "        return (\n",
        "            torch.FloatTensor(np.array(states)).to(device),\n",
        "            torch.FloatTensor(np.array(actions)).to(device),\n",
        "            torch.FloatTensor(np.array(rewards)).unsqueeze(1).to(device),\n",
        "            torch.FloatTensor(np.array(next_states)).to(device),\n",
        "            torch.FloatTensor(np.array(dones)).unsqueeze(1).to(device)\n",
        "        )"
      ],
      "metadata": {
        "id": "8Q1PTqcNhGhG"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"LunarLanderContinuous-v3\")\n",
        "obs_dim = env.observation_space.shape[0]\n",
        "act_dim = env.action_space.shape[0]\n",
        "action_low = env.action_space.low[0]\n",
        "action_high = env.action_space.high[0]\n",
        "\n",
        "actor = Actor(obs_dim, act_dim, action_low, action_high).to(device)\n",
        "critic = Critic(obs_dim, act_dim).to(device)\n",
        "critic_target = Critic(obs_dim, act_dim).to(device)\n",
        "critic_target.load_state_dict(critic.state_dict())\n",
        "\n",
        "actor_optim = optim.Adam(actor.parameters(), lr=ACTOR_LR)\n",
        "critic_optim = optim.Adam(critic.parameters(), lr=CRITIC_LR)\n",
        "\n",
        "replay_buffer = ReplayBuffer(REPLAY_SIZE)"
      ],
      "metadata": {
        "id": "RGQmmzSohIS2"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def update():\n",
        "    states, actions, rewards, next_states, dones = replay_buffer.sample(BATCH_SIZE)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        next_actions, log_probs = actor(next_states)\n",
        "        target_q1, target_q2 = critic_target(next_states, next_actions)\n",
        "        target_q = torch.min(target_q1, target_q2) - ALPHA * log_probs\n",
        "        target_q = rewards + GAMMA * (1 - dones) * target_q\n",
        "\n",
        "    current_q1, current_q2 = critic(states, actions)\n",
        "    critic_loss = F.mse_loss(current_q1, target_q) + F.mse_loss(current_q2, target_q)\n",
        "\n",
        "    critic_optim.zero_grad()\n",
        "    critic_loss.backward()\n",
        "    critic_optim.step()\n",
        "\n",
        "    for param in critic.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    actions_pred, log_probs_pred = actor(states)\n",
        "    q1_pred, q2_pred = critic(states, actions_pred)\n",
        "    actor_loss = (ALPHA * log_probs_pred - torch.min(q1_pred, q2_pred)).mean()\n",
        "\n",
        "    actor_optim.zero_grad()\n",
        "    actor_loss.backward()\n",
        "    actor_optim.step()\n",
        "\n",
        "    for param in critic.parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for param, target_param in zip(critic.parameters(), critic_target.parameters()):\n",
        "            target_param.data.copy_(TAU * param.data + (1 - TAU) * target_param.data)\n",
        "\n",
        "obs, _ = env.reset()\n",
        "episode_return = 0\n",
        "episode_length = 0"
      ],
      "metadata": {
        "id": "qyTQEqzshJou"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for step in range(1, TOTAL_STEPS + 1):\n",
        "    if step <= START_STEPS:\n",
        "        action = env.action_space.sample()\n",
        "    else:\n",
        "        action = actor.get_action(obs)\n",
        "\n",
        "    next_obs, reward, terminated, truncated, _ = env.step(action)\n",
        "    done = terminated or truncated\n",
        "    replay_buffer.add(obs, action, reward, next_obs, done)\n",
        "\n",
        "    obs = next_obs\n",
        "    episode_return += reward\n",
        "    episode_length += 1\n",
        "\n",
        "    if done:\n",
        "        obs, _ = env.reset()\n",
        "        print(f\"Step: {step}, Return: {episode_return:.2f}, Length: {episode_length}\")\n",
        "        episode_return = 0\n",
        "        episode_length = 0\n",
        "\n",
        "    if step >= UPDATE_AFTER and step % UPDATE_EVERY == 0:\n",
        "        for _ in range(UPDATE_EVERY):\n",
        "            update()\n",
        "\n",
        "env.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VcoXVAlphLVn",
        "outputId": "77d291a3-30d7-4572-bf2c-325c090b27a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step: 81, Return: -279.66, Length: 81\n",
            "Step: 195, Return: -213.62, Length: 114\n",
            "Step: 269, Return: -30.70, Length: 74\n",
            "Step: 419, Return: -132.30, Length: 150\n",
            "Step: 554, Return: -63.49, Length: 135\n",
            "Step: 653, Return: -316.96, Length: 99\n",
            "Step: 718, Return: -72.88, Length: 65\n",
            "Step: 824, Return: -294.72, Length: 106\n",
            "Step: 952, Return: -403.14, Length: 128\n",
            "Step: 1066, Return: -291.43, Length: 114\n",
            "Step: 1163, Return: -281.45, Length: 97\n",
            "Step: 1293, Return: -67.94, Length: 130\n",
            "Step: 1390, Return: -370.58, Length: 97\n",
            "Step: 1535, Return: -242.83, Length: 145\n",
            "Step: 1634, Return: -228.44, Length: 99\n",
            "Step: 1716, Return: -81.15, Length: 82\n",
            "Step: 1812, Return: -207.36, Length: 96\n",
            "Step: 1894, Return: -85.75, Length: 82\n",
            "Step: 1987, Return: -354.38, Length: 93\n",
            "Step: 2088, Return: -196.56, Length: 101\n",
            "Step: 2202, Return: -485.34, Length: 114\n",
            "Step: 2283, Return: -181.28, Length: 81\n",
            "Step: 2404, Return: -290.89, Length: 121\n",
            "Step: 2488, Return: -121.30, Length: 84\n",
            "Step: 2700, Return: -112.67, Length: 212\n",
            "Step: 2803, Return: -332.47, Length: 103\n",
            "Step: 2930, Return: -87.07, Length: 127\n",
            "Step: 3076, Return: -132.07, Length: 146\n",
            "Step: 3183, Return: -179.26, Length: 107\n",
            "Step: 3307, Return: -170.91, Length: 124\n",
            "Step: 3374, Return: -221.54, Length: 67\n",
            "Step: 3468, Return: -351.26, Length: 94\n",
            "Step: 3566, Return: -260.45, Length: 98\n",
            "Step: 3697, Return: -150.69, Length: 131\n",
            "Step: 3763, Return: -56.68, Length: 66\n",
            "Step: 3874, Return: -70.93, Length: 111\n",
            "Step: 3983, Return: -116.93, Length: 109\n",
            "Step: 4094, Return: -410.67, Length: 111\n",
            "Step: 4185, Return: -316.24, Length: 91\n",
            "Step: 4274, Return: -460.84, Length: 89\n",
            "Step: 4415, Return: -66.78, Length: 141\n",
            "Step: 4528, Return: -120.48, Length: 113\n",
            "Step: 4633, Return: -373.16, Length: 105\n",
            "Step: 4730, Return: -121.59, Length: 97\n",
            "Step: 4827, Return: -289.55, Length: 97\n",
            "Step: 4925, Return: -316.02, Length: 98\n",
            "Step: 5042, Return: -56.20, Length: 117\n",
            "Step: 5135, Return: -311.25, Length: 93\n",
            "Step: 5236, Return: -218.06, Length: 101\n",
            "Step: 5324, Return: -382.04, Length: 88\n",
            "Step: 5418, Return: -114.91, Length: 94\n",
            "Step: 5569, Return: -312.86, Length: 151\n",
            "Step: 5733, Return: -100.64, Length: 164\n",
            "Step: 5794, Return: -66.47, Length: 61\n",
            "Step: 5918, Return: -98.18, Length: 124\n",
            "Step: 6037, Return: -177.84, Length: 119\n",
            "Step: 6128, Return: -356.19, Length: 91\n",
            "Step: 6213, Return: -276.14, Length: 85\n",
            "Step: 6309, Return: -445.40, Length: 96\n",
            "Step: 6414, Return: -83.30, Length: 105\n",
            "Step: 6518, Return: -285.58, Length: 104\n",
            "Step: 6608, Return: -239.88, Length: 90\n",
            "Step: 6686, Return: -166.87, Length: 78\n",
            "Step: 6790, Return: -109.50, Length: 104\n",
            "Step: 6900, Return: -258.35, Length: 110\n",
            "Step: 7050, Return: -129.19, Length: 150\n",
            "Step: 7144, Return: -227.97, Length: 94\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}